{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# APM598: Homework 3 (03/26)\n",
    "## author: Jieshu Wang (jwang490@asu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. n-gram models\n",
    "## Ex 1.\n",
    "### Ex 1. a) \n",
    "- Load and tokenize the text attached ’Plato_Republic.txt’.\n",
    "- Put all the words in lower case to regroup words like ’The’ and ’the’.\n",
    "- Compute the total number of words N in the text and the number of unique words (size of the vocabulary).\n",
    "    - size of vocabulary is ___7544___."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the republic.     persons of the dialogue.  \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with open('./data/Plato_Republic.txt', mode='r', encoding='utf-8-sig') as my_file:\n",
    "    # encoding='utf-8-sig' is to remove '\\ufeff' in the text.\n",
    "    my_text = my_file.read().replace('\\n',' ')\n",
    "    my_text = my_text.lower()\n",
    "\n",
    "print(my_text[0:44])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['the', 'republic', '.', 'persons', 'of', 'the', 'dialogue', '.', 'socrates', ',']\n",
      "-- length of texts (words) :  136312\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Tokenize words\n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/Jieshu/PycharmProjects/APM598_Deep_Neural_Networks/APM598_notebook4/nltk_data\")\n",
    "\n",
    "my_text_tokenized = nltk.word_tokenize(my_text)\n",
    "print(my_text_tokenized[0:10])\n",
    "print(\"-- length of texts (words) : \",len(my_text_tokenized))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ex 1. b) \n",
    "- Build a uni-gram. \n",
    "- Deduce the 5 most common words with at least 8 characters. \n",
    "    - 'certainly', 'knowledge', 'injustice', 'therefore', 'question'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The 5 most frequent words with at least 8 characters are:\n",
      "[('certainly', 235), ('knowledge', 152), ('injustice', 111), ('therefore', 109), ('question', 99)]\n",
      "The number of unique words are 7544\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# build a uni-gram\n",
    "my_unigram = nltk.ngrams(my_text_tokenized, 1)\n",
    "freq_dist_uni = nltk.FreqDist(my_unigram)\n",
    "\n",
    "# deduce the 5 most common words with at least 8 characters\n",
    "sorted_freq_dist_uni = freq_dist_uni.most_common()\n",
    "sorted_8_character_words = [(i[0][0], i[1]) for i in sorted_freq_dist_uni if len(i[0][0]) >=8]\n",
    "print(\"The 5 most frequent words with at least 8 characters are:\")\n",
    "print(sorted_8_character_words[0:5])\n",
    "print(f'The number of unique words are {freq_dist_uni.B()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ex 1. c)\n",
    "- Build a bi-gram and define a function that given two words (x1, x2) compute the probability\n",
    ": $\\mathbb{P}(x_{t+1}|x_t) = \\frac{\\#\\{(x_t,x_{t+1})\\}}{\\#\\{(x_t)\\}\\}}$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.032217308907138344\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "my_bigram = nltk.ngrams(my_text_tokenized, 2)\n",
    "# define a function to compute the probability\n",
    "def prob_word_pair(x_1: str, x_2: str, text) ->float:\n",
    "    corpus = nltk.word_tokenize(text.lower())\n",
    "    unigram = nltk.ngrams(corpus, 1)\n",
    "    bigram = nltk.ngrams(corpus, 2)\n",
    "    freq_distribution_uni = nltk.FreqDist(unigram)\n",
    "    freq_distribution_bi = nltk.FreqDist(bigram)\n",
    "    n_x_1 = freq_distribution_uni[(x_1.lower(),)]\n",
    "    n_x_1_2 = freq_distribution_bi[(x_1.lower(), x_2.lower())]\n",
    "    prob = n_x_1_2/n_x_1\n",
    "    return prob\n",
    "\n",
    "print(prob_word_pair('I', 'think', my_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 1. d) \n",
    "- Deduce the so-called perplexity of the bi-gram model\n",
    "    - 37.89928782828084\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The perplexity of Plato's Republic is: \n",
      "37.89928782828084\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def compute_perplexity(text)->float:\n",
    "    corpus = nltk.word_tokenize(text.lower())\n",
    "    corpus_length = len(corpus)\n",
    "    unigram = nltk.ngrams(corpus, 1)\n",
    "    bigram = nltk.ngrams(corpus, 2)\n",
    "    freq_distribution_uni = nltk.FreqDist(unigram)\n",
    "    freq_distribution_bi = nltk.FreqDist(bigram)\n",
    "    product = 1\n",
    "    for i in range(corpus_length-1):\n",
    "        x_1 = corpus[i]\n",
    "        x_2 = corpus[i+1]\n",
    "        n_x_1 = freq_distribution_uni[(x_1.lower(),)]\n",
    "        n_x_1_2 = freq_distribution_bi[(x_1.lower(), x_2.lower())]\n",
    "        prob = n_x_1_2/n_x_1\n",
    "        product *= prob**(-1/(corpus_length-1))\n",
    "    perplexity = product\n",
    "    return  perplexity\n",
    "\n",
    "print(\"The perplexity of Plato's Republic is: \")\n",
    "print(compute_perplexity(my_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Recurrent Neural Networks\n",
    "## Ex 2. \n",
    "### Ex 2. a) find the output y1, . . . , y5 and deduce the predicted characters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the output y1, . . . , y5 and deduce the predicted characters\n",
      "--- 0---\n",
      "    input: h\n",
      "    y_1: tensor([[0.5170, 0.3533, 0.0526, 0.0770]], grad_fn=<SoftmaxBackward>)\n",
      "    output: h\n",
      "--- 1---\n",
      "    input: e\n",
      "    y_2: tensor([[0.3275, 0.3681, 0.2037, 0.1006]], grad_fn=<SoftmaxBackward>)\n",
      "    output: e\n",
      "--- 2---\n",
      "    input: l\n",
      "    y_3: tensor([[0.1762, 0.2410, 0.4063, 0.1765]], grad_fn=<SoftmaxBackward>)\n",
      "    output: l\n",
      "--- 3---\n",
      "    input: l\n",
      "    y_4: tensor([[0.0949, 0.1423, 0.5210, 0.2418]], grad_fn=<SoftmaxBackward>)\n",
      "    output: l\n",
      "--- 4---\n",
      "    input: o\n",
      "    y_5: tensor([[0.0940, 0.1092, 0.3790, 0.4178]], grad_fn=<SoftmaxBackward>)\n",
      "    output: o\n",
      "Predicted characters are: ['h', 'e', 'l', 'l', 'o']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from HW3_util import ModuleParamsVanillaRNN, VanillaRNN, letter_to_tensor, torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a_ex2a = torch.tensor([[1.0, -1.0, -0.5, 0.5],\n",
    "                       [1.0, 1.0, -0.5, -1]])\n",
    "r_ex2a = torch.tensor([[1.0, 0.0],\n",
    "                      [0.0, 1.0]])\n",
    "b_ex2a = torch.tensor([[1.0, 1.0],\n",
    "                       [0.5, 1.0],\n",
    "                       [-1.0, 0.0],\n",
    "                       [0.0, -0.5]])\n",
    "all_letters_ex2a = 'helo'\n",
    "input_letter_list = ['h', 'e', 'l', 'l', 'o']\n",
    "\n",
    "class VanillaRNNEx2a(VanillaRNN):\n",
    "    def __init__(self, module_params: ModuleParamsVanillaRNN):\n",
    "        super(VanillaRNNEx2a, self).__init__(module_params)\n",
    "        with torch.no_grad():\n",
    "            self._A.weight.data = a_ex2a\n",
    "            # self._A.bias.data.fill_(0)\n",
    "            self._R.weight.data = r_ex2a\n",
    "            # self._R.bias.data.fill_(0)\n",
    "            self._B.weight.data = b_ex2a\n",
    "            # self._B.bias.data.fill_(0)\n",
    "\n",
    "def predict_characters_ex2a(input_list: list, all_letters: str):\n",
    "    vanilla_rnn_params = ModuleParamsVanillaRNN(all_letters_base=all_letters)\n",
    "    my_vanilla_rnn_ex2a = VanillaRNNEx2a(vanilla_rnn_params)\n",
    "    h = my_vanilla_rnn_ex2a.init_hidden()\n",
    "    predicted_charaters_list = []\n",
    "    for i in range(len(input_list)):\n",
    "        input = input_list[i]\n",
    "        print(f'--- {i}---')\n",
    "        x = letter_to_tensor(input, all_letters)\n",
    "        y, h = my_vanilla_rnn_ex2a(x, h)\n",
    "        predicted_character = all_letters[y.argmax()]\n",
    "        print(f\"    input: {input}\")\n",
    "        print(f'    y_{i+1}: {y}')\n",
    "        print(f'    output: {predicted_character}')\n",
    "        predicted_charaters_list.append(predicted_character)\n",
    "    print(f'Predicted characters are: {predicted_charaters_list}')\n",
    "\n",
    "print('the output y1, . . . , y5 and deduce the predicted characters')\n",
    "predict_characters_ex2a(input_letter_list, all_letters_ex2a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ex 2. b) Find matrices A, R, B such that the predicted characters are ”olleh”.\n",
    "- A.weight: [[-1.6251, -0.6183,  0.7184,  1.0523], [-1.5250, -0.3831,  1.4286, -0.9764]]\n",
    "- R.weight: [[ 2.6304,  1.8039], [-1.3376, -0.0809]]\n",
    "- B.weight: [[ 2.6276, -1.8918], [ 2.9543,  2.2045], [-2.1664,  2.6917], [-1.7653, -2.3543]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "  input = hello\n",
      "  output = ooooo\n",
      "loss at step 0: 1.391243076324463\n",
      "  input = hello\n",
      "  output = ooeeh\n",
      "loss at step 60: 1.3292919158935548\n",
      "  input = hello\n",
      "  output = olheh\n",
      "loss at step 120: 1.1782100677490235\n",
      "  input = hello\n",
      "  output = olleh\n",
      "loss at step 180: 1.0148378372192384\n",
      "  input = hello\n",
      "  output = olleh\n",
      "loss at step 240: 0.873841381072998\n",
      "  input = hello\n",
      "  output = olleh\n",
      "loss at step 300: 0.827212905883789\n",
      "  input = hello\n",
      "  output = olleh\n",
      "loss at step 360: 0.8035863876342774\n",
      "  input = hello\n",
      "  output = olleh\n",
      "loss at step 420: 0.7893017292022705\n",
      "  input = hello\n",
      "  output = olleh\n",
      "loss at step 480: 0.779842472076416\n",
      "  input = hello\n",
      "  output = olleh\n",
      "loss at step 540: 0.7731913566589356\n",
      "<generator object Module.parameters at 0x132e26750>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from HW3_util import VanillaRNNTrainer, VanillaRNN, ModuleParamsVanillaRNN\n",
    "\n",
    "all_letters_ex2b = 'helo'\n",
    "training_x_ex2b = 'hello'\n",
    "training_y_ex2b = 'olleh'\n",
    "module_params_ex2b = ModuleParamsVanillaRNN(all_letters_base=all_letters_ex2b, chunk_size=5,\n",
    "                                            is_print_training=True, n_steps=600, printing_step=60)\n",
    "trainer_ex2b = VanillaRNNTrainer(module_params=module_params_ex2b, module=VanillaRNN,\n",
    "                                 training_x=training_x_ex2b, training_y=training_y_ex2b)\n",
    "print(trainer_ex2b.model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "_A.weight\n",
      "    Parameter containing:\n",
      "tensor([[-1.6251, -0.6183,  0.7184,  1.0523],\n",
      "        [-1.5250, -0.3831,  1.4286, -0.9764]], requires_grad=True)\n",
      "_R.weight\n",
      "    Parameter containing:\n",
      "tensor([[ 2.6304,  1.8039],\n",
      "        [-1.3376, -0.0809]], requires_grad=True)\n",
      "_B.weight\n",
      "    Parameter containing:\n",
      "tensor([[ 2.6276, -1.8918],\n",
      "        [ 2.9543,  2.2045],\n",
      "        [-2.1664,  2.6917],\n",
      "        [-1.7653, -2.3543]], requires_grad=True)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "result_model_parameters = trainer_ex2b.model.named_parameters()\n",
    "for name, params in result_model_parameters:\n",
    "    print(f'{name}')\n",
    "    print(f'{params}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 3. vanishing/exploding gradient\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}